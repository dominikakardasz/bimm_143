---
title: "class08"
author: "Dominika Kardasz"
date: "April 25th, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#K-means clustering 

Let's start with an example of runing **kmeans()** function

```{r}
# Generate some example data for clustering- rnorm generates random 30 points, here two groups 
tmp <- c(rnorm(30,-3), rnorm(30,3))
x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```

To do :
-Use the kmeans() function setting k to 2 and nstart=20
-Inspect/print the results
```{r}
km <- kmeans(x, centers=2, nstart=20)
km

```


>Q. How many points are in each cluster?
>A. 30 points in each 



>Q. What ‘component’ of your result object details
- cluster size? 

```{r}
km$size

```
- cluster assignment/membership? 
```{r}
km$cluster

```

- cluster center? "centers"
```{r}
km$centers

```

>Plot x colored by the kmeans cluster assignment and
add cluster centers as blue points

```{r}
#accesing 1 cluster to colour it 
plot(x, col=km$cluster)
# adding the centres of the clusters 
points(km$centers, pch=18, col=("blue"), cex=3)
```


##Hierarchical Clustering example

We must give the **hclust()** function a distance matrix not the raw data as input 

```{r}
#Distance matrix calculation 
d<- dist(x)

#clustering
hc<- hclust(d)

#plot results as a dendrogram
plot(hc)
```

```{r}
plot(hc)
abline(h=6, col="red")
```

```{r}
cutree(hc, h=6)
```

```{r}
cutree(hc, k=2)
```

```{r}
cutree(hc, k=3)
```


Another example, but more real life like with overlapping clusters of data
```{r}
# Step 1. Generate some example data for clustering
x <- rbind(
matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2), # c1
matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
matrix(c(rnorm(50, mean = 1, sd = 0.3), # c3
rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x) <- c("x", "y")

# Step 2. Plot the data without clustering
plot(x)

# Step 3. Generate colors for known clusters
# (just so we can compare to hclust results)
col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(x, col=col)
```

>Q. Use the dist(), hclust(), plot() and cutree()
functions to return 2 and 3 clusters

>Q. How does this compare to your known 'col' groups?


```{r}
#distance matrix calculation and clustering
hc <- hclust( dist(x))
#plot results as a dendrogram
plot(hc)
abline(h=2, col="red")
abline(h=2.8, col="blue")
```

```{r}
# give results for 2 and 3 clusters
gp2 <- cutree(hc, k=2)
gp3 <- cutree(hc, k=3)
```

```{r}
gp2
```

```{r}

gp3
```


```{r}
plot(x, col=gp2)

```

```{r}
plot(x, col=gp3)

```

```{r}
table(gp2)

```

```{r}
table(gp3)

```

```{r}
#comparing in a way anwsers from the two different gp's 
table(gp2, gp3)

```

#PCA: Principal Component Analysis

We will use **prcomp()** function for PCA

```{r}
## You can also download this file from the class website!
mydata <- read.csv("https://tinyurl.com/expression-CSV",
                   row.names=1)
#shows just first few row of data not all 
head(mydata, 10)

```

```{r}
nrow(mydata)
```

```{r}
ncol(mydata)
```

```{r}
colnames(mydata)
```

```{r}
##Transposong the matrix , because prcomp reads it differnetly and running pca
pca <- prcomp(t(mydata), scale. = T)
```


```{r}
## A basic PC1 vs PC2 2-D plot
plot(pca$x[,1], pca$x[,2], xlab = "PC1", ylab = "PC2")
```
Calculate the percent variance captures in each PC
```{r}
## Precent variance is often more informative to look at
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)

head(pca.var.per)
```

```{r}
barplot(pca.var.per, main="Scree Plot",
xlab="Principal Component", ylab="Percent Variation")
```

```{r}
## A vector of colors for wt and ko samples
colvec <- colnames(mydata)
colvec[grep("wt", colvec)] <- "red"
colvec[grep("ko", colvec)] <- "blue"
plot(pca$x[,1], pca$x[,2], col=colvec, pch=16,
xlab=paste0("PC1 (", pca.var.per[1], "%)"),
ylab=paste0("PC2 (", pca.var.per[2], "%)"))
```

PCA UK food exhercise
>Load the data 

```{r}
x <- read.csv("UK_foods.csv", row.names = 1)
head(x)
```

>Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
dim(x)
```


```{r}
##visualise the data as barplots
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))

```

>Q3: Changing what optional argument in the above barplot() function results in the following plot?
>A: **beside()** change to FALSE

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))

```

>Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?
>A: If sth is on  a diagonal that means that the food is same 

```{r}
pairs(x, col=rainbow(10), pch=16)

```

```{r}
# Use the prcomp() PCA function 
pca <- prcomp( t(x) )
summary(pca)

#cumulative proportion means that letds say 96% of the data is captured in 2 dimensions 

```

```{r}
mycols<- c("orange", "red", "blue", "darkgreen")

# Plot PC1 vs PC2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))

#**colnames** names the points with country names
text(pca$x[,1], pca$x[,2], colnames(x), col=mycols)

```

##Digging deeper (variable loadings )

```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
# plot tells which foods are more eaten in countries against another 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )

```

```{r}


```